{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b994b-cb03-4d9b-9e77-7bd259fc23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "1Ô∏è‚É£ What is K-Nearest Neighbors (KNN) and how does it work?\n",
    "KNN is a supervised learning algorithm used for classification and regression. It works by:\n",
    "\n",
    "Storing all the data points during training.\n",
    "\n",
    "For a new data point, it calculates the distance (like Euclidean distance) to all training points.\n",
    "\n",
    "It selects the K nearest neighbors and predicts the output:\n",
    "\n",
    "Classification: by majority voting.\n",
    "\n",
    "Regression: by averaging their values.\n",
    "\n",
    "2Ô∏è‚É£ What is the difference between KNN Classification and KNN Regression?\n",
    "Aspect\tKNN Classification\tKNN Regression\n",
    "Target Variable\tCategorical (e.g., labels like 'cat', 'dog')\tContinuous (e.g., price, temperature)\n",
    "Prediction\tBased on majority class of neighbors\tBased on average of neighbors' values\n",
    "Voting Strategy\tMode (most common label)\tMean (or weighted average)\n",
    "\n",
    "3Ô∏è‚É£ What is the role of the distance metric in KNN?\n",
    "The distance metric (like Euclidean, Manhattan, or Minkowski) determines which points are considered \"close\". It heavily impacts which neighbors are selected:\n",
    "\n",
    "Different metrics capture different notions of similarity.\n",
    "\n",
    "E.g., Euclidean works well when features are continuous; Manhattan may suit high-dimensional data better.\n",
    "\n",
    "4Ô∏è‚É£ What is the Curse of Dimensionality in KNN?\n",
    "In high dimensions:\n",
    "\n",
    "Distances become less meaningful (all points seem equidistant).\n",
    "\n",
    "KNN relies on distance to find neighbors, so its performance degrades.\n",
    "\n",
    "The more dimensions, the sparser the data becomes.\n",
    "\n",
    "5Ô∏è‚É£ How can we choose the best value of K in KNN?\n",
    "Use cross-validation:\n",
    "\n",
    "Try multiple values of K (e.g., 1 to 20).\n",
    "\n",
    "Choose K that minimizes validation error.\n",
    "\n",
    "Common heuristic:\n",
    "\n",
    "Odd values of K for binary classification.\n",
    "\n",
    "K = ‚àö(number of samples) is a rough starting point.\n",
    "\n",
    "6Ô∏è‚É£ What are KD Tree and Ball Tree in KNN?\n",
    "KD Tree:\n",
    "\n",
    "A binary tree that partitions the data by splitting along dimensions.\n",
    "\n",
    "Works well in low dimensions (up to ~20-30).\n",
    "\n",
    "Ball Tree:\n",
    "\n",
    "Partitions space into hyperspheres (balls).\n",
    "\n",
    "Better for high-dimensional or clustered data.\n",
    "\n",
    "They are used to speed up nearest-neighbor searches.\n",
    "\n",
    "7Ô∏è‚É£ When should you use KD Tree vs. Ball Tree?\n",
    "Aspect\tKD Tree\tBall Tree\n",
    "Data Dimensionality\tLow-dimensional (<30)\tHigher-dimensional (>30)\n",
    "Data Structure\tUniform, continuous data\tClustered, irregular data\n",
    "Performance\tFaster in low dimensions\tBetter in high dimensions\n",
    "\n",
    "8Ô∏è‚É£ What are the disadvantages of KNN?\n",
    "Slow prediction (needs to compute distance for all points).\n",
    "\n",
    "Sensitive to irrelevant features and feature scaling.\n",
    "\n",
    "Curse of dimensionality affects performance.\n",
    "\n",
    "Memory intensive (stores the entire dataset).\n",
    "\n",
    "Doesn‚Äôt handle missing values natively.\n",
    "\n",
    "9Ô∏è‚É£ How does feature scaling affect KNN?\n",
    "KNN is distance-based, so:\n",
    "\n",
    "If features have different scales, larger-scale features will dominate.\n",
    "\n",
    "Standardization (z-score) or Min-Max scaling is essential for KNN to perform well.\n",
    "\n",
    "üîü What is PCA (Principal Component Analysis)?\n",
    "PCA is a dimensionality reduction technique that:\n",
    "\n",
    "Transforms data into a new coordinate system (principal components).\n",
    "\n",
    "The first component captures the maximum variance, the second the next highest, and so on.\n",
    "\n",
    "Helps in visualization, noise reduction, and speeding up models.\n",
    "\n",
    "1Ô∏è‚É£1Ô∏è‚É£ How does PCA work?\n",
    "Steps:\n",
    "\n",
    "Center the data (subtract mean).\n",
    "\n",
    "Compute covariance matrix.\n",
    "\n",
    "Find eigenvectors and eigenvalues of covariance matrix.\n",
    "\n",
    "Select top-k principal components based on largest eigenvalues.\n",
    "\n",
    "Transform data into the new lower-dimensional space.\n",
    "\n",
    "1Ô∏è‚É£2Ô∏è‚É£ What is the geometric intuition behind PCA?\n",
    "Imagine a cloud of points in space:\n",
    "\n",
    "PCA finds the directions (axes) along which the data varies the most (principal components).\n",
    "\n",
    "These directions form a new rotated coordinate system.\n",
    "\n",
    "PCA projects the data onto this system, capturing as much variance as possible in fewer dimensions.\n",
    "\n",
    "1Ô∏è‚É£3Ô∏è‚É£ What are Eigenvalues and Eigenvectors in PCA?\n",
    "Eigenvectors: The directions (principal axes) along which variance is maximized.\n",
    "\n",
    "Eigenvalues: The amount of variance captured by each eigenvector.\n",
    "\n",
    "In PCA:\n",
    "\n",
    "We select eigenvectors corresponding to the largest eigenvalues.\n",
    "\n",
    "1Ô∏è‚É£4Ô∏è‚É£ What is the difference between Feature Selection and Feature Extraction?\n",
    "Aspect\tFeature Selection\tFeature Extraction\n",
    "Definition\tChoose a subset of original features\tCreate new features by combining existing ones\n",
    "Method Example\tSelect top-10 features based on importance\tPCA, LDA (reduce dimensionality)\n",
    "Nature\tRetains original features\tTransforms features\n",
    "\n",
    "1Ô∏è‚É£5Ô∏è‚É£ How do you decide the number of components to keep in PCA?\n",
    "Look at explained variance plot (scree plot).\n",
    "\n",
    "Choose the number of components that captures a desired percentage (e.g., 95%) of variance.\n",
    "\n",
    "Rule of thumb: Use elbow method.\n",
    "\n",
    "1Ô∏è‚É£6Ô∏è‚É£ Can PCA be used for classification?\n",
    "PCA itself is unsupervised.\n",
    "\n",
    "However, you can:\n",
    "\n",
    "Preprocess data with PCA to reduce dimensions.\n",
    "\n",
    "Then feed the reduced data into a classifier (like KNN, SVM).\n",
    "\n",
    "It can improve performance by reducing noise.\n",
    "\n",
    "1Ô∏è‚É£7Ô∏è‚É£ What are the limitations of PCA?\n",
    "Only captures linear relationships.\n",
    "\n",
    "May discard features important for classification but with low variance.\n",
    "\n",
    "Sensitive to scaling of features.\n",
    "\n",
    "Principal components are often hard to interpret.\n",
    "\n",
    "1Ô∏è‚É£8Ô∏è‚É£ How do KNN and PCA complement each other?\n",
    "PCA reduces dimensionality ‚Üí mitigates curse of dimensionality ‚Üí KNN performs better.\n",
    "\n",
    "PCA handles correlated features ‚Üí creates orthogonal features ‚Üí improves KNN‚Äôs distance calculations.\n",
    "\n",
    "Pipeline: PCA ‚Üí scaling ‚Üí KNN.\n",
    "\n",
    "1Ô∏è‚É£9Ô∏è‚É£ How does KNN handle missing values in a dataset?\n",
    "KNN doesn‚Äôt handle missing values natively. You can:\n",
    "\n",
    "Impute missing values (mean, median, KNN imputation).\n",
    "\n",
    "Or use a KNN imputer: find K nearest neighbors based on non-missing features and impute accordingly.\n",
    "\n",
    "2Ô∏è‚É£0Ô∏è‚É£ What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
    "Aspect\tPCA\tLDA\n",
    "Type\tUnsupervised\tSupervised\n",
    "Goal\tMaximize variance\tMaximize class separation\n",
    "Labels Needed?\tNo\tYes (class labels required)\n",
    "Use Case\tDimensionality reduction, visualization\tClassification, dimensionality reduction\n",
    "Criterion\tEigenvectors of covariance matrix\tEigenvectors of between-class/within-class scatter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a17165-a7a9-4a7a-8987-468d793f40ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "2Ô∏è‚É£1Ô∏è‚É£ Train a KNN Classifier on the Iris dataset and print model accuracy\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Accuracy\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "2Ô∏è‚É£2Ô∏è‚É£ Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Synthetic regression data\n",
    "X, y = make_regression(n_samples=200, n_features=5, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train KNN Regressor\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
    "knn_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = knn_reg.predict(X_test)\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "2Ô∏è‚É£3Ô∏è‚É£ Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy\n",
    "\n",
    "for metric in ['euclidean', 'manhattan']:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "    knn.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, knn.predict(X_test))\n",
    "    print(f\"Accuracy ({metric}): {acc:.2f}\")\n",
    "2Ô∏è‚É£4Ô∏è‚É£ Train a KNN Classifier with different values of K and visualize decision boundaries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reduce Iris to 2 features for visualization\n",
    "X_vis = X[:, :2]\n",
    "X_train_v, X_test_v, y_train_v, y_test_v = train_test_split(X_vis, y, test_size=0.2, random_state=42)\n",
    "\n",
    "ks = [1, 5, 15]\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "for i, k in enumerate(ks):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_v, y_train_v)\n",
    "\n",
    "    # Plot decision boundary\n",
    "    x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
    "    y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y)\n",
    "    plt.title(f\"K={k}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "2Ô∏è‚É£5Ô∏è‚É£ Apply Feature Scaling before training a KNN model and compare results with unscaled data\n",
    "\n",
    "# Unscaled KNN\n",
    "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_unscaled.fit(X_train, y_train)\n",
    "acc_unscaled = accuracy_score(y_test, knn_unscaled.predict(X_test))\n",
    "\n",
    "# Scaled KNN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "acc_scaled = accuracy_score(y_test, knn_scaled.predict(X_test_scaled))\n",
    "\n",
    "print(f\"Accuracy without scaling: {acc_unscaled:.2f}\")\n",
    "print(f\"Accuracy with scaling: {acc_scaled:.2f}\")\n",
    "2Ô∏è‚É£6Ô∏è‚É£ Train a PCA model on synthetic data and print the explained variance ratio for each component\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Synthetic data\n",
    "X_synth, _ = make_regression(n_samples=200, n_features=5, noise=10, random_state=42)\n",
    "\n",
    "# PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_synth)\n",
    "\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "2Ô∏è‚É£7Ô∏è‚É£ Apply PCA before training a KNN Classifier and compare accuracy with and without PCA\n",
    "\n",
    "# Without PCA\n",
    "knn_no_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_no_pca.fit(X_train_scaled, y_train)\n",
    "acc_no_pca = accuracy_score(y_test, knn_no_pca.predict(X_test_scaled))\n",
    "\n",
    "# With PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca.fit(X_train_pca, y_train)\n",
    "acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n",
    "\n",
    "print(f\"Accuracy without PCA: {acc_no_pca:.2f}\")\n",
    "print(f\"Accuracy with PCA: {acc_pca:.2f}\")\n",
    "2Ô∏è‚É£8Ô∏è‚É£ Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9], 'metric': ['euclidean', 'manhattan']}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n",
    "2Ô∏è‚É£9Ô∏è‚É£ Train a KNN Classifier and check the number of misclassified samples\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "misclassified = (y_test != y_pred).sum()\n",
    "print(f\"Number of misclassified samples: {misclassified}\")\n",
    "3Ô∏è‚É£0Ô∏è‚É£ Train a PCA model and visualize the cumulative explained variance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, len(cum_var)+1), cum_var, marker='o')\n",
    "plt.title(\"Cumulative Explained Variance by PCA\")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Variance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "3Ô∏è‚É£1Ô∏è‚É£ Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy\n",
    "\n",
    "for weight in ['uniform', 'distance']:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, weights=weight)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    acc = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
    "    print(f\"Accuracy (weights={weight}): {acc:.2f}\")\n",
    "3Ô∏è‚É£2Ô∏è‚É£ Train a KNN Regressor and analyze the effect of different K values on performance\n",
    "\n",
    "ks = [1, 3, 5, 10, 20]\n",
    "for k in ks:\n",
    "    knn_reg = KNeighborsRegressor(n_neighbors=k)\n",
    "    knn_reg.fit(X_train, y_train)\n",
    "    mse = mean_squared_error(y_test, knn_reg.predict(X_test))\n",
    "    print(f\"K={k}, MSE={mse:.2f}\")\n",
    "3Ô∏è‚É£3Ô∏è‚É£ Implement KNN Imputation for handling missing values in a dataset\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "# Create a dataset with missing values\n",
    "X_missing = X.copy()\n",
    "X_missing[::10] = np.nan  # Add NaNs every 10th row\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "X_imputed = imputer.fit_transform(X_missing)\n",
    "print(\"Imputed dataset (first 5 rows):\\n\", X_imputed[:5])\n",
    "3Ô∏è‚É£4Ô∏è‚É£ Train a PCA model and visualize the data projection onto the first two principal components\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.title(\"Data projected onto first two PCA components\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "3Ô∏è‚É£5Ô∏è‚É£ Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance\n",
    "\n",
    "for algo in ['kd_tree', 'ball_tree']:\n",
    "    knn = KNeighborsClassifier(algorithm=algo)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    acc = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
    "    print(f\"Accuracy (algorithm={algo}): {acc:.2f}\")\n",
    "3Ô∏è‚É£6Ô∏è‚É£ Train a PCA model on a high-dimensional dataset and visualize the Scree plot\n",
    "\n",
    "X_hd, _ = make_regression(n_samples=200, n_features=50, noise=5, random_state=42)\n",
    "X_hd_scaled = StandardScaler().fit_transform(X_hd)\n",
    "\n",
    "pca_hd = PCA()\n",
    "pca_hd.fit(X_hd_scaled)\n",
    "\n",
    "plt.plot(np.arange(1, 51), pca_hd.explained_variance_ratio_, marker='o')\n",
    "plt.title(\"Scree Plot\")\n",
    "plt.xlabel(\"Component Number\")\n",
    "plt.ylabel(\"Variance Explained\")\n",
    "plt.show()\n",
    "3Ô∏è‚É£7Ô∏è‚É£ Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred = knn.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "3Ô∏è‚É£8Ô∏è‚É£ Train a PCA model and analyze the effect of different numbers of components on accuracy\n",
    "\n",
    "for n in [1, 2, 3, 4]:\n",
    "    pca = PCA(n_components=n)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train_pca, y_train)\n",
    "    acc = accuracy_score(y_test, knn.predict(X_test_pca))\n",
    "    print(f\"Components={n}, Accuracy={acc:.2f}\")\n",
    "3Ô∏è‚É£9Ô∏è‚É£ Train a KNN Classifier with different leaf_size values and compare accuracy\n",
    "\n",
    "for leaf in [10, 30, 50, 70]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, leaf_size=leaf)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    acc = accuracy_score(y_test, knn.predict(X_test_scaled))\n",
    "    print(f\"Leaf Size={leaf}, Accuracy={acc:.2f}\")\n",
    "4Ô∏è‚É£0Ô∏è‚É£ Train a PCA model and visualize how data points are transformed before and after PCA\n",
    "\n",
    "X_2d = X[:, :2]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(X_2d[:,0], X_2d[:,1], c=y)\n",
    "plt.title(\"Original Data\")\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(StandardScaler().fit_transform(X))\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=y)\n",
    "plt.title(\"Data after PCA\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
